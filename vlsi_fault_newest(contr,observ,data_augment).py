# -*- coding: utf-8 -*-
"""vlsi_fault_newest(contr,observ,data augment).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_r9pkgfkhzPIXYShwMyUdBocMatej6R
"""

# STEP 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio                                                                                                                               !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html
!pip install torch_geometric
!pip install tqdm
!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html

# -*- coding: utf-8 -*-
"""
vlsi_fault_newest_modified.ipynb

This script is a modified version for VLSI fault detection using GNNs.
Modifications include:
1.  Enhanced Node Features: Added Betweenness Centrality and Clustering Coefficient.
2.  Data Augmentation: Handling of 'x' values in test patterns by generating multiple concrete examples.
"""
import os
import re
import pickle
import random
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Dataset, Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GATConv, PairNorm
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# --- Configuration ---
bench_dir = '/content/drive/My Drive/vlsi fault detection_new/iscas 85 benchmarks'
out_dir = bench_dir  # Save outputs in the same folder
bench_list = [f for f in os.listdir(bench_dir) if f.endswith('.bench')]

# --- STEP 1: BENCH PARSER (No Changes) ---
def parse_bench(filename):
    G = nx.DiGraph()
    with open(filename) as f:
        for lineno, line in enumerate(f, 1):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # PI
            m = re.match(r'INPUT\((\w+)\)', line)
            if m:
                net = m.group(1)
                G.add_node(net, kind='PI')
                continue
            # PO
            m = re.match(r'OUTPUT\((\w+)\)', line)
            if m:
                net = m.group(1)
                if net not in G:
                    G.add_node(net, kind='NET')
                G.nodes[net]['kind'] = 'PO'
                continue
            # Gate
            m = re.match(r'(\w+)\s*=\s*(\w+)\(([\w,\s]+)\)', line)
            if m:
                out_net, gate_type, inputs = m.group(1), m.group(2), m.group(3)
                inputs = [tok.strip() for tok in inputs.split(',')]
                gate_node = f"{gate_type}_{out_net}"
                G.add_node(gate_node, kind='GATE', type=gate_type)
                for inp in inputs:
                    if inp not in G:
                        G.add_node(inp, kind='NET')
                    G.add_edge(inp, gate_node)
                if out_net not in G:
                    G.add_node(out_net, kind='NET')
                G.add_edge(gate_node, out_net)
                continue
            print(f"Warning: Unparsed line {lineno}: {line}")
    return G

# --- STEP 2: Detect all unique gate types (No Changes) ---
ALL_GATETYPES = set()
for bench in bench_list:
    G = parse_bench(os.path.join(bench_dir, bench))
    for _, data in G.nodes(data=True):
        if data.get('kind') == 'GATE':
            ALL_GATETYPES.add(data['type'])
ALL_GATETYPES = sorted(ALL_GATETYPES)

# --- STEP 3: MODIFIED FEATURE BUILDER ---

import networkx as nx
import numpy as np
from collections import deque
from sklearn.preprocessing import StandardScaler

def compute_scoap_controllability(G, pi_nodes):
    CC0 = {n: float('inf') for n in G.nodes()}
    CC1 = {n: float('inf') for n in G.nodes()}
    for n in pi_nodes:
        CC0[n], CC1[n] = 1.0, 0.0
    for node in nx.topological_sort(G):
        data = G.nodes[node]
        if data.get('kind') == 'GATE':
            inputs = list(G.predecessors(node))
            if not inputs:
                continue
            gate = data.get('type', '').upper()
            c0 = [CC0[i] for i in inputs]
            c1 = [CC1[i] for i in inputs]
            if gate == 'AND':
                y0 = min(c0) + 1; y1 = sum(c1) + 1
            elif gate == 'OR':
                y0 = sum(c0) + 1; y1 = min(c1) + 1
            elif gate == 'NAND':
                y0 = sum(c1) + 1; y1 = min(c0) + 1
            elif gate == 'NOR':
                y0 = min(c1) + 1; y1 = sum(c0) + 1
            elif gate == 'NOT':
                y0 = CC1[inputs[0]] + 1; y1 = CC0[inputs[0]] + 1
            elif gate == 'BUF':
                y0 = CC0[inputs[0]] + 1; y1 = CC1[inputs[0]] + 1
            elif gate in ('XOR','XNOR'):
                if gate == 'XOR':
                    y0 = sum(c0) + 1; y1 = sum(c1) + 1
                else:
                    y0 = sum(c1) + 1; y1 = sum(c0) + 1
            else:
                y0 = (sum(c0) / len(c0)) + 1
                y1 = (sum(c1) / len(c1)) + 1
            CC0[node], CC1[node] = y0, y1
        elif data.get('kind') == 'NET':
            preds = [p for p in G.predecessors(node) if G.nodes[p].get('kind') == 'GATE']
            if preds:
                CC0[node], CC1[node] = CC0[preds[0]], CC1[preds[0]]
    return CC0, CC1

def compute_scoap_observability(G, po_nodes, CC0, CC1):
    CO0 = {n: float('inf') for n in G.nodes()}
    CO1 = {n: float('inf') for n in G.nodes()}
    for n in po_nodes:
        CO0[n] = CO1[n] = 0.0
    for node in reversed(list(nx.topological_sort(G))):
        data = G.nodes[node]
        if data.get('kind') == 'GATE':
            succs = list(G.successors(node))
            if not succs:
                continue
            out_net = succs[0]
            co0_out, co1_out = CO0[out_net], CO1[out_net]
            inputs = list(G.predecessors(node))
            gate = data.get('type', '').upper()
            for i in inputs:
                others = [j for j in inputs if j != i]
                if gate == 'AND':
                    o0 = co0_out + sum(CC1[j] for j in others) + 1; o1 = co1_out + 1
                elif gate == 'OR':
                    o0 = co0_out + 1; o1 = co1_out + sum(CC0[j] for j in others) + 1
                elif gate == 'NAND':
                    o0 = co0_out + 1; o1 = co1_out + sum(CC0[j] for j in others) + 1
                elif gate == 'NOR':
                    o0 = co0_out + sum(CC1[j] for j in others) + 1; o1 = co1_out + 1
                elif gate == 'NOT':
                    o0 = co1_out + 1; o1 = co0_out + 1
                elif gate == 'BUF':
                    o0 = co0_out + 1; o1 = co1_out + 1
                else:
                    o0 = co0_out + 1; o1 = co1_out + 1
                CO0[i] = min(CO0[i], o0)
                CO1[i] = min(CO1[i], o1)
        elif data.get('kind') == 'NET':
            preds = [p for p in G.predecessors(node) if G.nodes[p].get('kind') == 'GATE']
            if preds:
                CO0[node] = min(CO0[node], CO0[preds[0]])
                CO1[node] = min(CO1[node], CO1[preds[0]])
    return CO0, CO1

def compute_levels(G, pi_nodes, node_list, max_cap=None):
    level = {n: float('inf') for n in node_list}
    q = deque()
    for pi in pi_nodes:
        level[pi] = 0
        q.append(pi)
    while q:
        u = q.popleft()
        for v in G.successors(u):
            if level[v] > level[u] + 1:
                level[v] = level[u] + 1
                q.append(v)
    if max_cap is not None:
        for n in level:
            level[n] = min(level[n], max_cap)
    return level

def build_dataset_from_graph(G):
    node_list = list(G.nodes())
    idx_of = {n: i for i, n in enumerate(node_list)}
    pi_nodes = [n for n, d in G.nodes(data=True) if d.get('kind') == 'PI']
    po_nodes = [n for n, d in G.nodes(data=True) if d.get('kind') == 'PO']
    CC0, CC1 = compute_scoap_controllability(G, pi_nodes)
    CO0, CO1 = compute_scoap_observability(G, po_nodes, CC0, CC1)
    levels = compute_levels(G, pi_nodes, node_list)
    centrality = nx.betweenness_centrality(G)
    clustering = nx.clustering(G.to_undirected())

    feats = []
    for n in node_list:
        d = G.nodes[n]
        pi = 1 if d['kind'] == 'PI' else 0
        po = 1 if d['kind'] == 'PO' else 0
        onehot = ([1 if d['type'] == gt else 0 for gt in ALL_GATETYPES]
                  if d['kind'] == 'GATE' else [0] * len(ALL_GATETYPES))
        feats.append([
            pi, po,
            CC0.get(n, 0), CC1.get(n, 0),
            CO0.get(n, 0), CO1.get(n, 0),
            levels[n],
            centrality.get(n, 0),
            clustering.get(n, 0)
        ] + onehot)

    X = np.array(feats, dtype=np.float32)
    orig_X = X.copy()
    edges = [[idx_of[u], idx_of[v]] for u, v in G.edges()]
    edge_index = np.array(edges, dtype=np.int64).T

    # Replace NaNs
    X = np.nan_to_num(X, nan=0.0)
    pos_inf = np.isposinf(orig_X)
    neg_inf = np.isneginf(orig_X)

    # Percentile‚Äêbased capping
    for i in range(X.shape[1]):
        finite_vals = X[np.isfinite(X[:, i]), i]
        if finite_vals.size:
            low = np.percentile(finite_vals, 1)
            high = np.percentile(finite_vals, 99)
            X[pos_inf[:, i], i] = high
            X[neg_inf[:, i], i] = low

    # Standardize
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    return X, edge_index, node_list


# --- STEP 4: Process all .bench files (Calls modified feature builder) ---
for bench in bench_list:
    bench_path = os.path.join(bench_dir, bench)
    print(f"Processing {bench}...")
    G = parse_bench(bench_path)

    graph_file = os.path.join(out_dir, bench + '.gpickle')
    with open(graph_file, 'wb') as f:
        pickle.dump(G, f)

    X, edge_index, nodes = build_dataset_from_graph(G)

    base = os.path.splitext(bench)[0]
    np.save(os.path.join(out_dir, base + '_X.npy'), X)
    np.save(os.path.join(out_dir, base + '_edges.npy'), edge_index)
    print(f" ‚Üí Saved {base}_X.npy {X.shape}, {base}_edges.npy {edge_index.shape}")

# --- STEP 5: MODIFIED DATA AUGMENTATION & PARSING ---

def augment_pattern(inp_str, max_patterns=1):
    """
    Generates up to `max_patterns` concrete patterns from a pattern with 'x's.
    """
    if 'x' not in inp_str:
        return [inp_str]

    x_indices = [i for i, char in enumerate(inp_str) if char == 'x']
    generated_patterns = {inp_str.replace('x', '0')} # Start with one variation

    # Generate random variations
    for _ in range(max_patterns * 5): # Iterate more to ensure diversity
        if len(generated_patterns) >= max_patterns:
            break
        temp_list = list(inp_str)
        for i in x_indices:
            temp_list[i] = random.choice(['0', '1'])
        generated_patterns.add("".join(temp_list))

    return list(generated_patterns)[:max_patterns]

# --- Main Parsing Loop with Augmentation ---
targets = ['c1908'] # Circuits to re-run

for name in targets:
    bench_path = os.path.join(bench_dir, f'{name}.bench')
    test_path = None

    for f in os.listdir(bench_dir):
        if f.startswith(name) and f.endswith('_89.test'): # Using generic .test extension
            test_path = os.path.join(bench_dir, f)
            break
    if not test_path:
        print(f"‚ö†Ô∏è  No .test file found for {name}")
        continue

    with open(bench_path) as f:
        num_pi = sum(1 for L in f if re.match(r'\s*INPUT\(', L))
    print(f"‚Üí {name}: detected {num_pi} PIs from {name}.bench")

    gpickle = os.path.join(bench_dir, f'{name}.bench.gpickle')
    try:
        G = pickle.load(open(gpickle, 'rb'))
    except Exception as e:
        print(f"‚ö†Ô∏è  Couldn‚Äôt load graph {gpickle}: {e}")
        continue
    nodes = list(G.nodes())

    patterns, masks = [], []
    current_fault = None

   # ... inside the loop for name in targets:
    print(f"Parsing and augmenting test file: {test_path}")
    unmatched_lines = 0
    with open(test_path) as f:
        for line in tqdm(f):
            line = line.strip()
            if not line: continue

            # MODIFICATION: Changed \s+ to \s* to make space optional
            m_f = re.match(r'^(\S+)\s*/[01]', line)
            if m_f:
                current_fault = m_f.group(1).split('->')[-1]
                continue

            m_p = re.match(r'^\s*\d+:\s*([01x]+)\s+([01x]+)', line)
            if m_p and current_fault:
                inp = m_p.group(1)
                if len(inp) != num_pi:
                    print(f" ‚ö†Ô∏è Skipped {name}: got {len(inp)} inputs (expected {num_pi})")
                    continue

                augmented_patterns = augment_pattern(inp)
                for aug_pat_str in augmented_patterns:
                    patterns.append([int(c) for c in aug_pat_str])

                    mask = [0] * len(nodes)

# Map current_fault (net) to its driving gate for labeling
                    preds = [p for p in G.predecessors(current_fault)] if current_fault in G else []
                    driver_gate = preds[0] if preds else None

                    if driver_gate in nodes:
                        mask[nodes.index(driver_gate)] = 1
                    else:
    # If driver_gate not found, fallback to marking the net itself for safety
                        if current_fault in nodes:
                            mask[nodes.index(current_fault)] = 1
                        else:
                            print(f"‚ö†Ô∏è Fault '{current_fault}' and its driver gate not found in nodes.")
                    # This warning is important, but let's not print it for every pattern
                    # pass # Removed the else statement
                    masks.append(mask)
            # MODIFICATION: Added else for debugging
            else:
                # Ignore comments, but print other non-matching lines
                if not line.startswith('#'):
                    unmatched_lines += 1

    if unmatched_lines > 0:
        print(f"üîç Found {unmatched_lines} lines in {os.path.basename(test_path)} that did not match fault or pattern format.")
# ... rest of your code
        if not (m_p and current_fault):
                continue

        inp = m_p.group(1)
        if len(inp) != num_pi:
                print(f" ‚ö†Ô∏è Skipped {name}: got {len(inp)} inputs (expected {num_pi})")
                continue

            # MODIFICATION: Augment pattern and append data
        augmented_patterns = augment_pattern(inp)
        for aug_pat_str in augmented_patterns:
                patterns.append([int(c) for c in aug_pat_str])

                # NEW: Gate-level fault detection modification
                mask = [0] * len(nodes)

# Map current_fault (net) to its driving gate for labeling
                preds = [p for p in G.predecessors(current_fault)] if current_fault in G else []
                driver_gate = preds[0] if preds else None

                if driver_gate in nodes:
                    mask[nodes.index(driver_gate)] = 1
                else:
    # If driver_gate not found, fallback to marking the net itself for safety
                    if current_fault in nodes:
                        mask[nodes.index(current_fault)] = 1
                    else:
                        print(f"‚ö†Ô∏è Fault '{current_fault}' and its driver gate not found in nodes.")

                masks.append(mask)

    if patterns:
        P = np.array(patterns, dtype=np.int8)
        M = np.array(masks, dtype=np.int8)
        out_p = os.path.join(bench_dir, f'{name}_patterns_aug.csv')
        out_m = os.path.join(bench_dir, f'{name}_masks_aug.csv')
        np.savetxt(out_p, P, fmt='%d', delimiter=',')
        np.savetxt(out_m, M, fmt='%d', delimiter=',')
        print(f"‚úîÔ∏è  {name}: Wrote augmented data to {out_p} ({P.shape}), {out_m} ({M.shape})")
    else:
        print(f"‚ö†Ô∏è  {name}: no valid patterns parsed.")

import os
import pickle
import torch.nn.functional as F
import math
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

import torch
import torch.nn as nn
import torch.optim as optim

from torch_geometric.data import Data, Dataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GATConv, PairNorm

from torch.utils.data import Subset
from tqdm.auto import tqdm

# ---- Dataset ----
class OnTheFlyCircuitDataset(Dataset):
    def __init__(self, bench_dir, circuit, transform=None, pre_transform=None):
        super().__init__(transform=transform, pre_transform=pre_transform)
        gpath = os.path.join(bench_dir, f"{circuit}.bench.gpickle")
        with open(gpath, 'rb') as f:
            G = pickle.load(f)

        self.node_names = list(G.nodes())
        self.num_nodes = len(self.node_names)

        in_deg = np.array([G.in_degree(n) for n in self.node_names], dtype=np.float32).reshape(-1, 1)
        out_deg = np.array([G.out_degree(n) for n in self.node_names], dtype=np.float32).reshape(-1, 1)

        X_static = np.load(os.path.join(bench_dir, f"{circuit}_X.npy"))
        self.node_features = np.concatenate([X_static, in_deg, out_deg], axis=1)

        E = np.load(os.path.join(bench_dir, f"{circuit}_edges.npy"))
        self.edge_index = torch.tensor(E, dtype=torch.long)

        # load augmented patterns/masks
        pf = os.path.join(bench_dir, f"{circuit}_patterns_aug.csv")
        mf = os.path.join(bench_dir, f"{circuit}_masks_aug.csv")
        patterns = np.loadtxt(pf, delimiter=',', dtype=np.int8)
        masks    = np.loadtxt(mf, delimiter=',', dtype=np.int8)
        N, F     = masks.shape

        # reserve at least one sample per fault
        reserved = []
        for f in range(F):
            idxs = np.nonzero(masks[:, f])[0]
            if idxs.size:
                reserved.append(int(idxs[0]))

        all_idxs = set(range(N))
        remaining = np.array(list(all_idxs - set(reserved)), dtype=int)

        BUDGET = 55000
        need = max(0, BUDGET - len(reserved))
        if need > 0 and remaining.size > 0:
            extra = np.random.choice(remaining, size=min(need, remaining.size), replace=False)
        else:
            extra = np.array([], dtype=int)

        reserved_arr = np.array(reserved, dtype=int)
        selected = np.concatenate([reserved_arr, extra]) if reserved_arr.size or extra.size else np.array([], dtype=int)

        self.patterns = patterns[selected]
        self.masks    = masks[selected]

        del patterns, masks, reserved, remaining, extra, selected

    def __len__(self):
        return self.patterns.shape[0]

    def __getitem__(self, idx):
        pat = self.patterns[idx].astype(np.float32)
        mask = self.masks[idx].astype(np.float32)

        pat_tiled = np.tile(pat, (self.num_nodes, 1))
        feat = np.concatenate([self.node_features, pat_tiled], axis=1)

        return Data(x=torch.tensor(feat, dtype=torch.float),
                    edge_index=self.edge_index,
                    y=torch.tensor(mask, dtype=torch.float))


# --- STEP 7: GNN MODEL AND TRAINING (Modified with Dropout + Regularization + Early Stopping) ---
class CircuitGAT(nn.Module):
    def __init__(self, in_channels, hidden_channels=128, heads=4, dropout=0.2):
        super().__init__()
        self.dropout = dropout
        # Layer 1
        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)
        self.res1 = nn.Linear(in_channels, hidden_channels * heads)
        self.norm1 = PairNorm()
        # Layer 2
        self.gat2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)
        self.res2 = nn.Linear(hidden_channels * heads, hidden_channels)
        self.norm2 = PairNorm()
        # Layer 3
        self.gat3 = GATConv(hidden_channels, hidden_channels, heads=1, dropout=dropout)
        self.norm3 = PairNorm()
        self.fc = nn.Linear(hidden_channels, 1)

    def forward(self, x, edge_index):
        h1 = self.gat1(x, edge_index)
        h1 = self.norm1(h1)
        r1 = self.res1(x)
        h1 = torch.relu(h1 + r1)
        h1 = F.dropout(h1, p=self.dropout, training=self.training)

        h2 = self.gat2(h1, edge_index)
        h2 = self.norm2(h2)
        r2 = self.res2(h1)
        h2 = torch.relu(h2 + r2)
        h2 = F.dropout(h2, p=self.dropout, training=self.training)

        h3 = self.gat3(h2, edge_index)
        h3 = self.norm3(h3)
        h3 = torch.relu(h3 + h2)
        h3 = F.dropout(h3, p=self.dropout, training=self.training)

        return self.fc(h3).squeeze(-1)


# --- Training with progress bar ---
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    pbar = tqdm(loader, desc="Training", unit="batch")
    for batch in pbar:
        batch = batch.to(device)
        optimizer.zero_grad()
        logits = model(batch.x, batch.edge_index)
        loss = criterion(logits, batch.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * batch.num_graphs
        pbar.set_postfix(loss=f"{loss.item():.4f}")  # live loss update
    return total_loss / len(loader.dataset)

# --- Ranking Metrics ---
def precision_at_k(probs, true_idx, K):
    topk = np.argsort(probs)[-K:]
    return 1.0 if true_idx in topk else 0.0

# --- Evaluation ---
def eval_metrics(model, loader, criterion, device):
    model.eval()
    losses, all_probs, all_true = [], [], []
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            logits = model(batch.x, batch.edge_index)
            losses.append(criterion(logits, batch.y).item() * batch.num_graphs)
            probs = torch.sigmoid(logits).cpu().numpy()
            all_probs.append(probs)
            all_true.append(batch.y.cpu().numpy())

    avg_loss = sum(losses) / len(loader.dataset)
    p_flat = np.concatenate(all_probs)
    t_flat = np.concatenate(all_true)

    auc_score = 0.5 if np.sum(t_flat) == 0 else roc_auc_score(t_flat, p_flat)

    N = loader.dataset.dataset.num_nodes
    K = max(1, math.ceil(0.05 * N))
    P_at_K = []
    if p_flat.size > 0:
        for idx in range(len(p_flat) // N):
            p = p_flat[idx*N:(idx+1)*N]
            t = t_flat[idx*N:(idx+1)*N]
            true_idx = np.argmax(t)
            P_at_K.append(precision_at_k(p, true_idx, K))

    return avg_loss, auc_score, np.mean(P_at_K) if P_at_K else 0.0


# --- Main Execution Loop ---
if __name__ == '__main__':
    bench_dir = '/content/drive/My Drive/vlsi fault detection_new/iscas 85 benchmarks'
    circuits = ['c1908']
    for circuit in circuits:
        print(f"=== Running for circuit: {circuit} ===")
        hidden, lr, batch_size = 128, 1e-3, 128
        epochs, patience = 100, 4  # patience reduced to 4
        min_delta = 0.002      # min improvement required
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        ds = OnTheFlyCircuitDataset(bench_dir, circuit)
        idx = np.arange(len(ds))
        trainval, test_idx = train_test_split(idx, test_size=0.2, random_state=42)

        trainval = np.random.choice(trainval, min(50000, len(trainval)), replace=False)
        test_idx = np.random.choice(test_idx, min(5000, len(test_idx)), replace=False)
        train_idx, val_idx = train_test_split(trainval, test_size=0.1, random_state=42)

        from torch.utils.data import Subset
        train_loader = DataLoader(Subset(ds, train_idx), batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(Subset(ds, val_idx), batch_size=batch_size)
        test_loader = DataLoader(Subset(ds, test_idx), batch_size=batch_size)

        model = CircuitGAT(ds[0].x.shape[1], hidden_channels=hidden, dropout=0.2).to(device)
        pos_weight = torch.tensor([(ds.num_nodes - 1) / 1.0], device=device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3
        )

        best_val_p5, wait = -1, 0
        hist = {'tr_loss': [], 'val_loss': [], 'tr_auc': [], 'val_auc': [], 'tr_p5': [], 'val_p5': []}

        for ep in range(1, epochs + 1):
            print(f"\n=== Epoch {ep}/{epochs} ===")
            tr_loss_epoch = train_epoch(model, train_loader, optimizer, criterion, device)
            tr_loss, tr_auc, tr_p5 = eval_metrics(model, train_loader, criterion, device)
            val_loss, val_auc, val_p5 = eval_metrics(model, val_loader, criterion, device)

            scheduler.step(val_p5)

            hist['tr_loss'].append(tr_loss); hist['val_loss'].append(val_loss)
            hist['tr_auc'].append(tr_auc); hist['val_auc'].append(val_auc)
            hist['tr_p5'].append(tr_p5); hist['val_p5'].append(val_p5)

            print(f"Epoch {ep} | Tr L {tr_loss:.4f} AUC {tr_auc:.4f} P@5% {tr_p5:.4f} | "
                  f"Val L {val_loss:.4f} AUC {val_auc:.4f} P@5% {val_p5:.4f}")

            # Early stopping based on P@5% with tolerance
            if val_p5 > best_val_p5 + min_delta:
                best_val_p5 = val_p5
                wait = 0
                torch.save(model.state_dict(), f"best_{circuit}_aug.pth")
                print("‚Üí Saved new best model.")
            else:
                wait += 1
                print(f"No improvement in P@5% (wait {wait}/{patience})")

            if wait >= patience:
                print(f"Early stopping for {circuit} (best P@5% = {best_val_p5:.4f})\n")
                break

        print("\n--- Final Evaluation on Test Set ---")
        model.load_state_dict(torch.load(f"best_{circuit}_aug.pth"))
        test_loss, test_auc, test_p5 = eval_metrics(model, test_loader, criterion, device)
        print(f"\nTest L {test_loss:.4f} AUC {test_auc:.4f} P@5% {test_p5:.4f}")


        epochs_range = range(1, len(hist['tr_loss']) + 1)
        plt.figure(figsize=(18, 5))
        plt.subplot(1, 3, 1)
        plt.plot(epochs_range, hist['tr_loss'], label='Train Loss')
        plt.plot(epochs_range, hist['val_loss'], label='Val Loss')
        plt.legend()
        plt.title('Loss vs. Epochs')

        plt.subplot(1, 3, 2)
        plt.plot(epochs_range, hist['tr_auc'], label='Train AUC')
        plt.plot(epochs_range, hist['val_auc'], label='Val AUC')
        plt.legend()
        plt.title('AUC vs. Epochs')

        plt.subplot(1, 3, 3)
        plt.plot(epochs_range, hist['tr_p5'], label='Train P@5%')
        plt.plot(epochs_range, hist['val_p5'], label='Val P@5%')
        plt.legend()
        plt.title('Precision@5% vs. Epochs')

        plt.tight_layout()
        plt.show()